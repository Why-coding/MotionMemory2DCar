# -*- coding: utf-8 -*-
"""MotionMemory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16uCZ_hX9vJAj-JRX8uWy-CMRRmAyGAkh
"""

from google.colab import drive

drive.mount('\content\drive')

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/ResearchData/motion/TrainFinal/Data/PathCurves/curves.zip
# !unzip /content/train_data.zip

import os
import torch
import torchvision
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from PIL import Image
import json
from torchvision.transforms import transforms
import random
import torch
import torchvision
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.models import resnet18
import torch.nn as nn
import torch.nn.functional as F
import zipfile
import matplotlib.pyplot as plt
import numpy as np
from tqdm.notebook import tqdm
import torch.optim as optim
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from numpy import arange

PATH2 = "/content/drive/MyDrive/ResearchData/motion/TrainFinal/Data/PathCurves/"
PATH = "/content/curves/"
# PATH = "/content/train_data/"
# PATH = "/content/trap/"
# PATH = "/content/random/"

torch.manual_seed(2020)
np.random.seed(2020)
random.seed(2020)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if device.type == "cuda":
    torch.cuda.get_device_name()

embedding_dims = 30
batch_size = 64
epochs = 60
no_class = 40

data = pd.read_csv(PATH+"dataCurves4.csv")
train_df, test_df = train_test_split(data, test_size=0.1, train_size=0.9)
train_df, validation_df = train_test_split(train_df, test_size=0.1, train_size=0.9)
train_df= train_df.reset_index(drop= True)
test_df = test_df.reset_index(drop= True)
validation_df = validation_df.reset_index(drop= True)
test_label = test_df.iloc[:, 1].values
# validation_label = validation_df.iloc[:, 1].values

# for i in range(14,60):
  # print(test_df.loc[i].at['ImageName'])
# print(test_df.iloc[12:1012, 1])

# temp_df = train_df[train_df['Label'] == 2].reset_index(drop=True)
# print(temp_df)

temp_df = test_df['ImageName']
temp_df.to_csv('test_curves.txt', index= False, header = False)

class MNIST(Dataset):
    def __init__(self, df, path, train=True, transform=None):
        self.data_csv = df
        self.is_train = train
        self.transform = transform
        self.path = path
#         self.to_pil = transforms.ToPILImage()

        if self.is_train:
            self.images = df.iloc[:, 0].values
            self.labels = df.iloc[:, 1].values
            self.index = df.index.values
        else:
            self.images = df.iloc[:, 0].values

    def __len__(self):
        return len(self.images)

    def __getitem__(self, item):

        anchor_image_name = self.images[item]
        anchor_idx = anchor_image_name.find(":")
        anchor_image_path = " "
        # anchor_image_path = self.path + anchor_image_name.split(":")[1].split("_")[0]+ '/' + anchor_image_name
        # anchor_image_path = self.path +anchor_image_name.split("_")[1]+ '/' + anchor_image_name
        if anchor_idx > 0:
          anchor_image_path = self.path +anchor_image_name.split(":")[1].split("_")[0]+ '/' + anchor_image_name
        else:
          anchor_image_path = self.path +anchor_image_name.split("_")[1]+ '/' + anchor_image_name
        ##### Anchor Image #######
        anchor_img = Image.open(anchor_image_path).convert('1')
        if self.is_train:
            anchor_label = self.labels[item]
            positive_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]
            positive_item = random.choice(positive_list)
            positive_image_name = self.images[positive_item]
            positive_idx = positive_image_name.find(":")
            positive_image_path = " "
            if positive_idx > 0:
              positive_image_path = self.path +positive_image_name.split(":")[1].split("_")[0]+ '/' + positive_image_name
            else:
              positive_image_path = self.path +positive_image_name.split("_")[1]+ '/' + positive_image_name
            # positive_image_path = self.path +positive_image_name.split(":")[1].split("_")[0]+ '/'  + positive_image_name
            # positive_image_path = self.path +positive_image_name.split("_")[1]+ '/' + positive_image_name
            positive_img = Image.open(positive_image_path).convert('1')
            #positive_img = self.images[positive_item].reshape(28, 28, 1)
            negative_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]
            negative_item = random.choice(negative_list)
            negative_image_name = self.images[negative_item]
            negative_idx = negative_image_name.find(":")
            negative_image_path = " "
            if negative_idx > 0:
              negative_image_path = self.path +negative_image_name.split(":")[1].split("_")[0]+ '/' + negative_image_name
            else:
              negative_image_path = self.path +negative_image_name.split("_")[1]+ '/' + negative_image_name

            # negative_image_path = self.path +negative_image_name.split("_")[1]+ '/' + negative_image_name
            # negative_image_path = self.path +negative_image_name.split(":")[1].split("_")[0]+ '/'  + negative_image_name
            negative_img = Image.open(negative_image_path).convert('1')
            #negative_img = self.images[negative_item].reshape(28, 28, 1)
            if self.transform!=None:
                anchor_img = self.transform(anchor_img)
                positive_img = self.transform(positive_img)
                negative_img = self.transform(negative_img)
            return anchor_img, positive_img, negative_img, anchor_label
        else:
            if self.transform:
                anchor_img = self.transform(anchor_img)
            return anchor_img

train_ds = MNIST(train_df, PATH,
                 train=True,
                 transform=transforms.Compose([
                     transforms.ToTensor()
                 ]))
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)

validation_ds = MNIST(validation_df,PATH, train=True, transform=transforms.ToTensor())
validation_loader = DataLoader(validation_ds, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True)

test_ds = MNIST(test_df,PATH, train=False, transform=transforms.ToTensor())
test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4,pin_memory=True)

class TripletLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(TripletLoss, self).__init__()
        self.margin = margin

    def calc_euclidean(self, x1, x2):
        return (x1 - x2).pow(2).sum(1)

    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:
        distance_positive = self.calc_euclidean(anchor, positive)
        distance_negative = self.calc_euclidean(anchor, negative)
        losses = torch.relu(distance_positive - distance_negative + self.margin)

        return losses.mean()

class Network(nn.Module):
    def __init__(self, emb_dim=30):
        super(Network, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 8, 3),
            nn.PReLU(),
            nn.MaxPool2d(2, stride=2),
            #nn.Dropout(0.3),
            nn.Conv2d(8, 16, 3),
            nn.PReLU(),
            nn.MaxPool2d(2, stride=2),
            #nn.Dropout(0.3)
            nn.Conv2d(16, 32, 3),
            nn.PReLU(),
            nn.MaxPool2d(2, stride=2)
            # nn.Conv2d(32, 64, 3),
            # nn.PReLU()
        )

        self.fc = nn.Sequential(
            nn.Linear(32*4*4, 128),
            nn.PReLU(),
            nn.Linear(128, 64),
            nn.PReLU(),
            nn.Linear(64, emb_dim)
        )

    def forward(self, x):
        x = self.conv(x)
        # print(x.shape)
        x = x.view(-1, 32*4*4)
        x = self.fc(x)
        # x = nn.functional.normalize(x)
        return x

def init_weights(m):
    if isinstance(m, nn.Conv2d):
        torch.nn.init.kaiming_normal_(m.weight)

model = Network(embedding_dims)
model.apply(init_weights)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
# criterion = TripletLoss()
criterion = nn.TripletMarginLoss(margin=1.0, p=2)

def extract_embeddings(model, loader):
    embeddings = []
    labels = []
    with torch.no_grad():
        for data, label in tqdm(loader):
            embedding = model(data)
            embeddings.append(embedding.cpu().numpy())
            labels.append(label.cpu().numpy())
    return np.vstack(embeddings), np.hstack(labels)

def compute_clusters(embeddings):
  kmeans = KMeans(n_clusters=10, random_state= 0).fit(embeddings)
  return kmeans.cluster_centers_

def compute_accuracy(val_loader, centers):
    correct = 0
    total = 0
    with torch.no_grad():
        for img, _, _, label in val_loader:
            images, labels = img.to(device), label.to(device)
            outputs = model(images)
            distances = ((outputs.unsqueeze(1) - centers)**2).sum(-1) # compute L2 norm
            predictions = distances.argmin(1)
            total += labels.size(0)
            correct += (predictions == labels).sum().item()
    return 100 * correct / total

from sklearn import neighbors
model.train()
min = 1.0
max = 0.0
loss_all = []
for epoch in tqdm(range(epochs), desc="Epochs"):
    running_loss = []
    for step, (anchor_img, positive_img, negative_img, anchor_label) in enumerate(tqdm(train_loader, desc="Training", leave=False)):
        anchor_img = anchor_img.to(device)
        positive_img = positive_img.to(device)
        negative_img = negative_img.to(device)

        optimizer.zero_grad()
        anchor_out = model(anchor_img)
        positive_out = model(positive_img)
        negative_out = model(negative_img)

        loss = criterion(anchor_out, positive_out, negative_out)
        loss.backward()
        optimizer.step()

        running_loss.append(loss.cpu().detach().numpy())
    print("Epoch: {}/{} - Loss: {:.4f}".format(epoch+1, epochs, np.mean(running_loss)))
    loss_all.append(np.mean(running_loss))

    validation_results = []
    validation_labels = []
    total_correct = 0
    total_instances = 0

    # model.eval()
    # # knn = KNeighborsClassifier(n_neighbors= 5)
    # with torch.no_grad():
    #     for img, _, _, label in tqdm(train_loader):
    #         output = model(img.to(device))
    #         validation_results.append(output.cpu().numpy())
    #         validation_labels.append(label)

    # validation_results = np.vstack(validation_results)
    # validation_labels = np.hstack(validation_labels)

    # kmeans = KMeans(n_clusters=10, random_state=0).fit(validation_results)
    # predicted_labels = np.argmin(np.linalg.norm(kmeans.cluster_centers_[:, None] - validation_results[None, :], axis=-1), axis=0)
    # accuracy = accuracy_score(validation_labels, predicted_labels) *100

            # output = model(img.to(device))
            # validation_results.extend(output.cpu().numpy())
            # validation_labels.extend(label.cpu().numpy())


            # output = model(img.to(device))
            # # np.append(validation_results, output.cpu().numpy())
            # validation_results.append(output.cpu().numpy())
            # validation_labels.append(label)
            # tq = label.to(device)
            # # print(model(img.to(device)).cpu().numpy().shape)
            # predictions = output.argmax(dim=1, keepdim = True)
            # # _, predictions = torch.min(model(img.to(device)).data, 1)
            # # print(predictions)
            # # correct_predictions = sum(predictions==tq).item()
            # # total_correct+=correct_predictions
            # total_correct += predictions.eq(tq.view_as(predictions)).sum().item()
            # total_instances+=len(img)

    # accuracy = (total_correct/len(train_loader.dataset))*100


    # knn.fit(validation_results, validation_labels)
    # prediction = knn.predict(validation_results)
    # accuracy = accuracy_score(validation_labels, prediction) * 100



    # validation_results = np.concatenate(validation_results)
    # validation_labels = np.concatenate(validation_labels)
    # kmeans = KMeans(n_clusters=len(np.unique(validation_labels))).fit(validation_results)
    # pred_labels = kmeans.predict(validation_results)
    # accuracy = accuracy_score(validation_labels, pred_labels) * 100


    # print("Correct: ", total_correct, " Total: ", len(train_loader.dataset))
    # print("Accuracy: {:.3F}".format(accuracy))
    # print(predicted_labels)


    # if max< accuracy:
    #   max = accuracy
    #   torch.save({"model_state_dict2": model.state_dict(),
    #         "optimzier_state_dict2": optimizer.state_dict()
    #        }, PATH+"trained_model_curves_knn.pt")

    #   sm = torch.jit.script(model)
    #   sm.save(PATH+"trained_model_curves_knn_cpp.pt")

    if min>=np.mean(running_loss):
      min = np.mean(running_loss)
      torch.save({"model_state_dict": model.state_dict(),
            "optimzier_state_dict": optimizer.state_dict()
           }, PATH+"trained_model_curves4_new.pt")

      sm = torch.jit.script(model)
      sm.save(PATH+"trained_model_curves4_new_cpp.pt")


print(min)
print(max)

epochs = range(1, 61)
print(epochs)
# Plot and label the training and validation loss values
plt.plot(epochs, loss_all, label='Training Loss')
# plt.plot(epochs, val_values, label='Validation Loss')

# Add in a title and axes labels
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

# Set the tick locations
plt.xticks(arange(0, 59, 2))

# Display the plot
plt.legend(loc='best')
plt.show()

train_results = []
labels = []
total_correct = 0
total_instances = 0

for i in range(0,no_class):
  train_results.append([])

model.eval()
with torch.no_grad():
    for img, _, _, label in tqdm(train_loader):
        temp_label = label.cpu().numpy()
        temp = model(img.to(device)).cpu().numpy()
        labels.append(label)
        for i in range(0, len(temp_label)):
          train_results[temp_label[i]].append(temp[i])

cluster_centroid = []
for i in range (0, no_class):
  cluster_centroid.append(np.mean(train_results[i],axis=0))

print(np.array(cluster_centroid).shape)

test_results = []
predicted_labels = []

model.eval()
with torch.no_grad():
    for img in tqdm(test_loader):
        output = model(img.to(device))
        test_results.append(output.cpu().numpy())

test_results = np.vstack(test_results)
# print(np.linalg.norm(kmeans.cluster_centers_[:, None] - test_results[None, :], axis=-1))
# predicted_labels = np.argmin(np.linalg.norm(kmeans.cluster_centers_[:, None] - test_results[None, :], axis=-1), axis=0)
for i in test_results:
  # print(test_results[i])
  predicted_labels.append(np.argmin(np.linalg.norm(cluster_centroid - i, axis=-1), axis=0))

correct = 0
for i in range(len(test_loader)):
  if(test_label[i] == predicted_labels[i]):
    correct+=1
accuracy = correct*100/len(predicted_labels)

print(accuracy)
print(predicted_labels)
print(test_label)

np.savetxt('cluster_curves4.txt', cluster_centroid, delimiter=',')